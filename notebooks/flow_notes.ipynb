{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Notes/instructions on how to run experiments.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Notes/instructions on how to run experiments.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nna.fileUtils import list_files,save_to_csv\n",
    "\n",
    "from pathlib import Path\n",
    "from nna.pre_process_func import read_queue\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# originally from ../src/notes.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* start with screen\n",
    "* get GPU Memory so I can use it later\n",
    "* one file file at a time (~2 gb)\n",
    "  * I will divide files to 1 hour slices\n",
    "  * I will use 25 cpus so 25 processes\n",
    "  * embeddings file name will be filename_embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) create a text file with paths to input audio files (one full path per line)\n",
    " \n",
    "* `find /tank/data/nna/real/ -iname \"*flac\" > ExperimentRunVXXX.txt`\n",
    "* or following code, list_files function\n",
    "\n",
    "v2) since now we have new files in the old folders, we need to find which files has results which files do not,\n",
    "we could let the code ignore existing results however, ffmpeg divides files into couple of hours of segments without checking results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafiles_path=\"/home/enis/projects/nna/data/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_path=\"/tank/data/nna/real/\"\n",
    "ignore_folders=[\"/tank/data/nna/real/ivvavik/\",\"/tank/data/nna/real/stinchcomb/dups/\",\"/tank/data/nna/real/stinchcomb/excerpts/\"]\n",
    "\n",
    "files_path_list=list_files(search_path,ignore_folders)\n",
    "files_path_list=list(set(files_path_list))\n",
    "\n",
    "datafiles_path=\"/home/enis/projects/nna/data/\"\n",
    "input_filename = \"ExperimentRunV5.txt\"\n",
    "\n",
    "thepath = datafiles_path + input_filename\n",
    "with open(thepath,\"w\") as myfile:\n",
    "    myfile.writelines(\"\\n\".join(files_path_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) update input, output directory, input list,  in /nna/src/params.py\n",
    "\n",
    "3-1) sync files \n",
    "\n",
    "`rsync -av --recursive --update ./ enis@crescent:/home/enis/projects/nna/ `\n",
    "\n",
    "3-2) run codes\n",
    "```bash\n",
    "python3 ./src/scripts/pre_process.py &>> job_logs/logs.txt; python /home/enis/projects/nna/src/nna/slack_message.py -t \"cpu job ended\" -f job_logs/logs.txt &\n",
    "\n",
    "python3 ./src/scripts/watch_VGGish.py &>> job_logs/logs_gpu.txt; python /home/enis/projects/nna/src/nna/slack_message.py -t \"gpu job stopped\" & \n",
    "```\n",
    "4) to re-run, update the code and remove temporary flac files\n",
    "\n",
    "`rsync -av --recursive --update /Users/berk/Documents/workspace/speech_audio_understanding/src/ enis@crescent:/home/enis/projects/nna/`\n",
    "\n",
    "`find /scratch/enis/data/nna/real/ -iname \"*flac\"  -delete`\n",
    "\n",
    "`find /scratch/enis/data/nna/real/ -path \"*/*_segments*/*\" -delete`\n",
    "\n",
    "`find /scratch/enis/data/nna/real/ -name \"*_segments\" -type d -delete`\n",
    "\n",
    "also remove `job_logs/pre_processing_queue.csv` if jobs left unfinished\n",
    "\n",
    "and STOP all processes\n",
    "\n",
    "5) tracking progress \n",
    "```bash\n",
    "cat job_logs/pre_processing_queue.csv | wc -l; cat job_logs/pre_processed_queue.csv | wc -l; cat job_logs/VGGISH_processing_queue.csv | wc -l; cat job_logs/vggish_embeddings_queue.csv | wc -l;\n",
    "```\n",
    "6) backup\n",
    "```bash\n",
    "tar cf - /scratch/enis/data/nna/backup/NUI_DATA/ -P | pv -s $(du -sb /scratch/enis/data/nna/backup/NUI_DATA/ | awk '{print $1}') | gzip > embeddings_backup.tar.gz\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1=read_queue(\"../src/job_logs/pre_processing_queue.csv\")\n",
    "q2=read_queue(\"../src/job_logs/pre_processed_queue.csv\")\n",
    "q3=read_queue(\"../src/job_logs/VGGISH_processing_queue.csv\")\n",
    "q4=read_queue(\"../src/job_logs/vggish_embeddings_queue.csv\")\n",
    "q5=read_queue(\"../src/job_logs/Audioset_processing_queue.csv\")\n",
    "q6=read_queue(\"../src/job_logs/Audioset_output_queue.csv\")\n",
    "\n",
    "CABLE=read_queue(\"../src/job_logs/_CABLE_output_queue.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1set=set([\"/\".join(i.split(\"/\")[-1:])[:-5] for i in q1])\n",
    "q2set=set([\"/\".join(i.split(\"/\")[-2:-1])[:-13] for i in q2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SINP-05_20190709_090000', 'S4A10252_20190529_171602', 'S4A10259_20190514_183000'}\n"
     ]
    }
   ],
   "source": [
    "print(q1set.difference(q2set))\n",
    "print(q2set.difference(q1set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.64% is NOT finished\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3rk9ayjc-V1-0-0-0\n",
      "4.13% is NOT finished\n"
     ]
    }
   ],
   "source": [
    "# files without results (subDirectoryAddons)\n",
    "# this one goes through files in the properties_df and checks if there are corresponding files with given file name add ons.\n",
    "\n",
    "from nna.fileUtils import standard_path_style\n",
    "import pandas as pd\n",
    "\n",
    "# file_properties_df_ST=pd.read_pickle(\"../data/stinchcomb_dataV1.pkl\")\n",
    "file_properties_df_ST = []\n",
    "# file_properties_df = pd.read_pickle(\"../data/realdata_v2No_stinchcomb.pkl\")\n",
    "# file_properties_df = pd.read_pickle(\"../data/prudhoeAndAnwr4photoExp_dataV1.pkl\")\n",
    "# file_properties_df = pd.read_pickle(\"/home/enis/projects/nna/data/prudhoeAndAnwr4photoExp_dataV1.pkl\")\n",
    "file_properties_df = pd.read_pickle('/scratch/enis/data/nna/database/allFields_dataV5.pkl')\n",
    "\n",
    "# subDirectoryAddon=\"vgg\"\n",
    "# subDirectoryAddon=\"_CABLE\"\n",
    "# subDirectoryAddon=\"_CABLE\"\n",
    "# subDirectoryAddons=[\"CABLE\",\"RUNNINGWATER\",\"INSECT\", \"RAIN\", \"WATERBIRD\", \"WIND\", \"SONGBIRD\", \"AIRCRAFT\"]\n",
    "subDirectoryAddons=[\"3rk9ayjc-V1-0-0-0\"]\n",
    "notDoneFilesDict={}\n",
    "for subDirectoryAddon in subDirectoryAddons:\n",
    "    print(subDirectoryAddon)\n",
    "    notDoneFiles=[]\n",
    "    \n",
    "    for afile,row in file_properties_df.iterrows():\n",
    "        checkFile = standard_path_style(\"/scratch/enis/data/nna/real/\",row,sub_directory_addon=subDirectoryAddon,file_name_addon=\"\")\n",
    "        if not checkFile.exists():\n",
    "            notDoneFiles.append(afile)\n",
    "    \n",
    "    if file_properties_df_ST != []:\n",
    "        for afile,row in file_properties_df_ST.iterrows():\n",
    "            checkFile = standard_path_style(\"/scratch/enis/data/nna/real/\",row,sub_directory_addon=subDirectoryAddon,file_name_addon=\"\")\n",
    "            if not checkFile.exists():\n",
    "        #         print(afile)\n",
    "    #             print(checkFile)\n",
    "                notDoneFiles.append(afile)\n",
    "\n",
    "    notDoneFiles.sort()\n",
    "    notDoneFiles=[str(i) for i in notDoneFiles]\n",
    "    notDoneFilesDict[subDirectoryAddon]=notDoneFiles[:]\n",
    "\n",
    "\n",
    "# input_filename = \"ExperimentRunV6.txt\"\n",
    "\n",
    "# thepath=datafiles_path+input_filename\n",
    "# with open(thepath,\"w\") as myfile:\n",
    "#     myfile.writelines(\"\\n\".join(notDoneFiles))\n",
    "total_input_file_count = len(file_properties_df)\n",
    "count_not_done_files = len(notDoneFiles)\n",
    "print(f'{(count_not_done_files/total_input_file_count)*100:.2f}% is NOT finished')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asd\n"
     ]
    }
   ],
   "source": [
    "print('asd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"77.62% \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13729"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(notDoneFiles) 13729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4937"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(noEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/enis/projects/nna/data/inferenceInputs/allModels6tagRunV1_20191102.txt'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = \"allModels6tagRunV1_20191102.txt\"\n",
    "\n",
    "thepath=datafiles_path+\"inferenceInputs/\"+input_filename\n",
    "# thepath\n",
    "with open(thepath,\"w\") as myfile:\n",
    "    myfile.writelines(\"\\n\".join([str(i) for i in yesEmbeddings]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from missing results, find missing embeddings\n",
    "# in the next part, we find missing embeddings by searching folders, here with by iterating file_properties_df\n",
    "noEmbeddings=[]\n",
    "yesEmbeddings=[]\n",
    "for f in notDoneFiles:\n",
    "    row = file_properties_df.loc[Path(f)]\n",
    "    checkFile= standard_path_style(\"/scratch/enis/data/nna/real/\",row,subDirectoryAddon=\"_vgg\",fileNameAddon=\"_rawembeddings000.npy\")\n",
    "    if not checkFile.exists():\n",
    "#         print(checkFile)\n",
    "        noEmbeddings.append(checkFile)\n",
    "    else:\n",
    "        yesEmbeddings.append(checkFile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(noEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any of the missing files were in the previous experiment\n",
    "inputFilesOld=\"/scratch/enis/data/nna/real/input_2020-10-12_17:11:26.txt\"\n",
    "with open(inputFilesOld) as ff:\n",
    "    ffL=ff.readlines()\n",
    "    ffL = [i.strip().split(\"\\t\")[0] for i in ffL]\n",
    "\n",
    "noEmbeddings2=[]\n",
    "yesEmbeddings2=[]\n",
    "for f in notDoneFiles:\n",
    "    row = file_properties_df.loc[Path(f)]\n",
    "    checkFile= standard_path_style(\"/scratch/enis/data/nna/real/\",row,subDirectoryAddon=\"_vgg\",fileNameAddon=\"_rawembeddings000.npy\")\n",
    "    if f not in ffL:\n",
    "#         print(checkFile)\n",
    "        noEmbeddings2.append(checkFile)\n",
    "    else:\n",
    "        yesEmbeddings2.append(checkFile)\n",
    "\n",
    "setnoEmbeddings = set(noEmbeddings)\n",
    "setnoEmbeddings2 = set(noEmbeddings2)\n",
    "print(\"Following ones are in the input but do not have embeddings\")\n",
    "setnoEmbeddings.difference(setnoEmbeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4934"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(setnoEmbeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=[]\n",
    "for i in yesEmbeddings[0:10]:\n",
    "    t.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in yesEmbeddings[-20:-10]:\n",
    "    t.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIND missing prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anwr 12388\n",
      "prudhoe 13148\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "inputCSV=[]\n",
    "# for i in [\"ivvavik\",\"anwr\",  \"prudhoe\" , \"stinchcomb\"]:\n",
    "for i in [\"anwr\",  \"prudhoe\" ]:\n",
    "    x=glob.glob( f\"/scratch/enis/data/nna/real/{i}/**/*_rawembeddings*.npy\",recursive=True)\n",
    "    print(i,len(x))\n",
    "    inputCSV.extend(x)\n",
    "\n",
    "\n",
    "# save_to_csv(\"../src/job_logs/inferenceRun3.csv\",[[str(afile)] for afile in inputCSV])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(\"/home/enis/projects/nna/job_logs/inferenceRunVtest.csv\",[[str(afile)] for afile in t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _preprocessed.npy\n",
    "search_path=\"/tank/data/nna/real/\"\n",
    "ignore_folders=[\"/tank/data/nna/real/ivvavik/\",\"/tank/data/nna/real/stinchcomb/dups/\",\"/tank/data/nna/real/stinchcomb/excerpts/\"]\n",
    "\n",
    "files_path_list=list_files(search_path,ignore_folders)\n",
    "files_path_list=list(set(files_path_list))\n",
    "\n",
    "datafiles_path=\"/home/enis/projects/nna/data/\"\n",
    "input_filename = \"ExperimentRunV5.txt\"\n",
    "\n",
    "thepath=datafiles_path+input_filename\n",
    "with open(thepath,\"w\") as myfile:\n",
    "    myfile.writelines(\"\\n\".join(files_path_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO all these are running on raw embeddings, makes sure thats checked before running on the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputCsvFile=\"./job_logs/inferenceRun3.csv\"\n",
    "\n",
    "inputCsvFile=\"/home/enis/projects/nna/data/inferenceInputs/allModels6tagRunV1_20191102.txt\"\n",
    "outputLogs=\"job_logs/MLlogs.txt\"\n",
    "i=0\n",
    "python3 ./src/scripts/watchMLInference.py --inputCsv $inputCsvFile --modelPath \"./src/nna/assets/sklearnModels/Cable_Gaussian Process_Raw_Concat_2020-03-02--14-06.joblib\"  --modelName \"_CABLE\" &>> $outputLogs & \n",
    "pids[i]=$! && ((i=i+1));\n",
    "python3 ./src/scripts/watchMLInference.py --inputCsv $inputCsvFile --modelPath \"./src/nna/assets/sklearnModels/Running Water_Neural Net_Raw_Concat_2020-03-02--14-06.joblib\"  --modelName \"_RUNNINGWATER\" &>> $outputLogs &\n",
    "pids[i]=$! && ((i=i+1));\n",
    "python3 ./src/scripts/watchMLInference.py --inputCsv $inputCsvFile --modelPath \"./src/nna/assets/sklearnModels/Insect_Linear SVM_Raw_Concat_2020-03-02--14-06.joblib\"  --modelName \"_INSECT\" &>> $outputLogs & \n",
    "pids[i]=$! && ((i=i+1));\n",
    "python3 ./src/scripts/watchMLInference.py --inputCsv $inputCsvFile --modelPath \"./src/nna/assets/sklearnModels/Rain_Gaussian Process_Raw_Concat_2020-03-02--14-06.joblib\"  --modelName \"_RAIN\" &>> $outputLogs &\n",
    "pids[i]=$! && ((i=i+1));\n",
    "python3 ./src/scripts/watchMLInference.py --inputCsv $inputCsvFile --modelPath \"./src/nna/assets/sklearnModels/Water Bird_RBF SVM_Raw_many2one_2020-03-02--14-06.joblib\"  --modelName \"_WATERBIRD\" &>> $outputLogs &\n",
    "pids[i]=$! && ((i=i+1));\n",
    "python3 ./src/scripts/watchMLInference.py --inputCsv $inputCsvFile --modelPath \"./src/nna/assets/sklearnModels/Wind_Neural Net_Raw_many2one_2020-03-02--14-06.joblib\"  --modelName \"_WIND\" &>> $outputLogs &\n",
    "pids[i]=$! && ((i=i+1));\n",
    "python3 ./src/scripts/watchMLInference.py --inputCsv $inputCsvFile --modelPath \"./src/nna/assets/sklearnModels/Songbird_Neural Net_Raw_many2one_2020-03-02--14-06.joblib\"  --modelName \"_SONGBIRD\" &>> $outputLogs &\n",
    "pids[i]=$! && ((i=i+1));\n",
    "python3 ./src/scripts/watchMLInference.py --inputCsv $inputCsvFile --modelPath \"./src/nna/assets/sklearnModels/Aircraft_AdaBoost_Raw_Average_2020-03-02--14-06.joblib\"  --modelName \"_AIRCRAFT\" &>> $outputLogs &\n",
    "pids[i]=$! && ((i=i+1));\n",
    "for pid in ${pids[*]}; do\n",
    "    wait $pid\n",
    "done;\n",
    "python3 slack_message.py -t \"MLInference job ended\" -f $outputLogs &\n",
    "\n",
    "\n",
    "    AIRCRAFT,SONGBIRD,WATERBIRD,WIND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputCSV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-f88a6673f602>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogsPath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../src/job_logs/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0minputCSVset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCSV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubDirectoryAddon\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubDirectoryAddons\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcsvFile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogsPath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msubDirectoryAddon\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_output_queue.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inputCSV' is not defined"
     ]
    }
   ],
   "source": [
    "# set(inputCSV)\n",
    "import time\n",
    "logsPath=\"../src/job_logs/\"\n",
    "inputCSVset=set(inputCSV)\n",
    "for subDirectoryAddon in subDirectoryAddons:\n",
    "    csvFile=logsPath+subDirectoryAddon+\"_output_queue.csv\"\n",
    "    print(subDirectoryAddon)\n",
    "    csvFileSet=set(read_queue(csvFile))\n",
    "    print(len(inputCSVset.difference(csvFileSet)))\n",
    "#     for i in inputCSVset.difference(csvFileSet):\n",
    "#         print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speechEnv",
   "language": "python",
   "name": "speechenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
