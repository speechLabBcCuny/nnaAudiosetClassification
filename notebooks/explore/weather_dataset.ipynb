{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "133f7854-7bae-4f70-9080-005e3c1be00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7bc70c14984e56ab4f1c4f6896a310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import importlib\n",
    "path='/home/enis/projects/nna/src/scripts/'\n",
    "os.chdir(path)\n",
    "\n",
    "import teacher\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ebe5c77-30cb-47ac-a2ff-167e18cbdee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(teacher)\n",
    "\n",
    "version_tag='weather_V1'\n",
    "config = teacher.setup(versiontag=version_tag)\n",
    "\n",
    "weather_data_folder='/scratch/enis/data/nna/weather_data/2017_2020'\n",
    "\n",
    "root_path = '/scratch/enis/data/nna/labeling/samples'\n",
    "config['split_out_path'] = f'{root_path}/{version_tag}/audio_'\n",
    "\n",
    "## data-1: as many sampes as possible\n",
    "config['new_dataset_path'] = f'{root_path}/{version_tag}/{version_tag}.csv'\n",
    "config['dataset_version'] = 'W1'\n",
    "# config['versiontag'] = 'yfitloiq-V1'\n",
    "config['excell_label_headers']=['day_length','air_temp','snow_depth',\n",
    "        'cloud_fraction','relative_humidity','runoff','rain_precip',\n",
    "         'snow_precip','wind_direction','wind_speed']\n",
    "\n",
    "# config['excell_label_headers']=['day_length','air_temp','snow_depth',\n",
    "#         'cloud_fraction','relative_humidity','runoff','rain_precip',\n",
    "#          'snow_precip','wind_direction','wind_speed',\n",
    "#         'snow_blowing_ground','snow_blowing_air',]\n",
    "        \n",
    "\n",
    "\n",
    "config['upper_taxo_links']={}\n",
    "\n",
    "FILE_PER_LOCATION=200\n",
    "# print('total sample count to be produced:',file_per_location*len(short_ones.keys()))\n",
    "\n",
    "# 40 Prudhoe or ANWR monitoring sites AND the Ivvavik sites\n",
    "short_input_csv_headers = ['day_length','air_temp','snow_depth',\n",
    "        'cloud_fraction','relative_humidity','runoff','rain_precip',\n",
    "         'snow_precip','wind_direction','wind_speed']\n",
    "# (year,month,day,hour,day_length,air_temp,snow_depth,\n",
    "#                 cloud_fraction,relative_humidity,runoff,rain_precip,\n",
    "#                     snow_precip,wind_direction,wind_speed)=row\n",
    "# for Dalton and Dempster\n",
    "long_input_csv_headers = ['day_length','air_temp','snow_depth',\n",
    "        'cloud_fraction','relative_humidity','runoff','rain_precip',\n",
    "         'snow_precip','total_precip','wind_direction','wind_speed',\n",
    "        'snow_blowing_ground','snow_blowing_air']\n",
    "\n",
    "\n",
    "# (year,month,day,hour,day_length,air_temp,snow_depth,\n",
    "# cloud_fraction,relative_humidity,runoff,rain_precip,\n",
    "# snow_precip,total_precip,wind_direction,wind_speed,\n",
    "# snow_blowing_ground,snow_blowing_air)=row\n",
    "\n",
    "short_locations=('prudhoe','ivvavik','anwr')\n",
    "long_locations=('dalton','dempster')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e61a6-75e6-4228-b2c0-d5623f8f59ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "781f0ef5-3e02-46f4-9abd-64dc85a125b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_properties_df = pd.read_pickle(\n",
    "    '/scratch/enis/data/nna/database/allFields_dataV10.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35759d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def csv_path_per_regloc(data_folder):\n",
    "    \n",
    "    station_csv={}\n",
    "    for region_path in glob.glob(f'{data_folder}/*'):\n",
    "        locations=glob.glob(region_path+'/sm_products_by_station/*')\n",
    "        region = Path(region_path).name.lower()\n",
    "        for location_path in locations:\n",
    "            location=Path(location_path).stem.split('_')[-1]\n",
    "            if region!=location[:-2].lower():\n",
    "                print(region,location)\n",
    "            location=location[-2:]\n",
    "            # print(location_path)\n",
    "            if region=='ivvavik':\n",
    "                location='SINP'+location\n",
    "            station_csv[(region,location)]=location_path\n",
    "            # print(region,location)\n",
    "        # print(len(locations))\n",
    "    return station_csv\n",
    "\n",
    "def year_per_regloc(station_csv,file_properties_df):\n",
    "\n",
    "    station_years={}\n",
    "    for region,location in station_csv.keys():\n",
    "        region_filtered=file_properties_df[file_properties_df['region']==region]\n",
    "        loc_reg_filtered = region_filtered[region_filtered['locationId']==location]\n",
    "        \n",
    "        # print(region,location)\n",
    "        unique_years = (loc_reg_filtered.year.unique())\n",
    "        unique_years = [int(year) for year in unique_years if int(year)>2018]\n",
    "        # print(unique_years)\n",
    "        station_years[(region,location)] = unique_years\n",
    "    return station_years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e5a0b15-a123-4a56-9e9d-34214061332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_csv = csv_path_per_regloc(weather_data_folder)\n",
    "station_years = year_per_regloc(station_csv,file_properties_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a6f6e3a-3380-46f7-bff5-a1f99de1b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region='dalton'\n",
    "# location='10'\n",
    "# region_filtered=file_properties_df[file_properties_df['region']==region]\n",
    "# loc_reg_filtered = region_filtered[region_filtered['locationId']==location]\n",
    "# loc_reg_filtered # 2019-07-23_10:30:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f69fbba5-855a-40e8-9b6b-012a3124fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rows(csv_fname,region,short_locations,long_locations):\n",
    "    with open(csv_fname, newline='') as csvfile:\n",
    "        # reader = csv.DictReader(csvfile,fieldnames=fieldnames)\n",
    "        csv_reader = list(csv.reader(csvfile))\n",
    "\n",
    "        short = True if len(csv_reader[0])==14 else False\n",
    "        if region in short_locations and not short:\n",
    "            raise Exception('short location has long csv {},{}'.format(region,location))\n",
    "        if region in long_locations and short:\n",
    "            raise Exception('long location has short csv {},{}'.format(region,location))\n",
    "\n",
    "        return csv_reader,short\n",
    "\n",
    "def get_random_rows(reader,file_per_location,station_years):\n",
    "        reader = [row for row in reader if int(row[0]) in station_years[(region,location)]]\n",
    "        rows_picked = random.choices(reader,k=file_per_location)\n",
    "        return rows_picked\n",
    "\n",
    "def parse_rows(rows_picked,location,region,short,short_headers,long_headers):\n",
    "\n",
    "    input_csv_headers = short_headers if short else long_headers\n",
    "\n",
    "    pd_rows=[]\n",
    "    for row in rows_picked:\n",
    "        year, month, day, hour = [int(row[x]) for x in range(4)]\n",
    "\n",
    "        pd_row={}\n",
    "        pd_row['location']=location\n",
    "        pd_row['region']=region.lower()\n",
    "        timestamp = datetime.datetime(year, month, day, hour=hour)\n",
    "        # timestamps represent timeperiod starting from 3 hours earlier\n",
    "        # we place them in the middle for finding best representation in audio\n",
    "        timestamp = timestamp - datetime.timedelta(hours=1.5)\n",
    "        pd_row['TIMESTAMP']= timestamp.strftime('%Y-%m-%d_%H:%M:%S')\n",
    "        row[4:] = [float(x) for x in row[4:]]\n",
    "        for label,data in zip(input_csv_headers,row[4:]):\n",
    "            pd_row[label]=data\n",
    "        pd_rows.append(pd_row)\n",
    "    return pd_rows\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f3d151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2019-06-10_07:30:00\n",
    "\n",
    "# filtered=file_properties_df[file_properties_df['region']=='dalton']\n",
    "# filtered = filtered[filtered['locationId']=='10']\n",
    "# filtered=filtered[filtered['timestamp']> datetime.datetime(2019,6,17,0,30,0)]\n",
    "# filtered=filtered[filtered['timestamp']< datetime.datetime(2019,6,18,0,0,0)]\n",
    "# filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba09e52-4330-4fe0-9664-acc0850f25b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_csv=[]\n",
    "missing_csv=[]\n",
    "for (region,location),fname in station_csv.items():\n",
    "    csv_reader,short = load_rows(fname,region,short_locations,long_locations)\n",
    "    rows_picked = get_random_rows(csv_reader,FILE_PER_LOCATION,station_years)\n",
    "\n",
    "    pd_rows = parse_rows(rows_picked,location,region,short,short_input_csv_headers,long_input_csv_headers)\n",
    "\n",
    "\n",
    "    selected_data=pd.DataFrame.from_dict(pd_rows)\n",
    "    new_dataset_csv, not_found_rows = teacher.generate_new_dataset(\n",
    "        selected_data,\n",
    "        config['versiontag'],\n",
    "        config['split_out_path'],\n",
    "        file_properties_df,\n",
    "        config['upper_taxo_links'],\n",
    "        config['dataset_version'],\n",
    "        buffer=3600,\n",
    "        excell_label_headers= {},#config['excell_label_headers'],\n",
    "        labels_thresholds= {},#config['labels_thresholds'],\n",
    "        outputSuffix='.wav',\n",
    "        dry_run=False,\n",
    "        excell_labels_2_names= {}, #config['excell_labels_2_names'],\n",
    "        stereo2mono=True,\n",
    "        overwrite=False,\n",
    "        sampling_rate=48000,\n",
    "        label_row_by_threshold=False,\n",
    "    )\n",
    "    dataset_csv.extend(new_dataset_csv)\n",
    "    missing_csv.extend(not_found_rows)\n",
    "\n",
    "\n",
    "# from copy import deepcopy\n",
    "# new_dataset_csv_backup = deepcopy(new_dataset_csv)\n",
    "# not_found_rows_backup = deepcopy(not_found_rows)\n",
    "\n",
    "# del not_found_rows_backup,new_dataset_csv_backup\n",
    "\n",
    "\n",
    "# write_csv(config['new_dataset_path'],\n",
    "#           new_dataset_csv,\n",
    "#           fieldnames=config['excell_all_headers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be48a4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3811"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff35e617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3700\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "dataset_csv_backup = deepcopy(dataset_csv)\n",
    "missing_csv_backup = deepcopy(missing_csv)\n",
    "\n",
    "excell_all_headers = [\n",
    "    'data_version', 'Annotator', 'Site ID', 'File Name', 'Date', 'Start Time',\n",
    "    'End Time', 'Length', 'Clip Path', 'Comments', 'weather_timestamp',\n",
    "    'region', 'day_length', 'air_temp', 'snow_depth', 'cloud_fraction',\n",
    "    'relative_humidity', 'runoff', 'rain_precip', 'snow_precip',\n",
    "    'wind_direction', 'wind_speed'\n",
    "]\n",
    "\n",
    "for row in dataset_csv_backup:\n",
    "    for k,v in row['weather_data'].items():\n",
    "        if k not in ['location','TIMESTAMP']:\n",
    "            row[k]=v\n",
    "        row['weather_timestamp'] = row['weather_data']['TIMESTAMP']\n",
    "    del row['weather_data']\n",
    "\n",
    "\n",
    "for row in dataset_csv_backup:\n",
    "    for k in list(row.keys()):\n",
    "        if k not in excell_all_headers:\n",
    "            del row[k]\n",
    "\n",
    "dataset_csv_unique = {}\n",
    "for row in dataset_csv_backup:\n",
    "    dataset_csv_unique[row['Clip Path']] = row\n",
    "dataset_csv_backup=list(dataset_csv_unique.values())\n",
    "\n",
    "print(len(dataset_csv_backup))\n",
    "teacher.write_csv(config['new_dataset_path'],\n",
    "          dataset_csv_backup,\n",
    "          fieldnames=excell_all_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcd6c84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_version': 'W1',\n",
       " 'Annotator': 'weather_V1',\n",
       " 'Site ID': 'SINP03',\n",
       " 'File Name': '/tank/data/nna/real/ivvavik/SINP03/2019/SINP-03_20190513_231602.flac',\n",
       " 'Start Time': '00:29:50.000000',\n",
       " 'End Time': '00:30:00.000000',\n",
       " 'Date': '05/14/2019',\n",
       " 'Length': '00:00:10.000000',\n",
       " 'Clip Path': PosixPath('/scratch/enis/data/nna/labeling/samples/weather_V1/audio_weather_V1/ivvavik/SINP03/SINP-03_20190513_231602_73m_48s__73m_58s.wav'),\n",
       " 'Comments': '',\n",
       " 'weather_timestamp': '2019-05-14_01:30:00',\n",
       " 'region': 'ivvavik',\n",
       " 'day_length': 20.2996368,\n",
       " 'air_temp': 0.922241211,\n",
       " 'snow_depth': 0.450530827,\n",
       " 'cloud_fraction': 0.831612885,\n",
       " 'relative_humidity': 84.5013275,\n",
       " 'runoff': 0.0,\n",
       " 'rain_precip': 3.24604343e-06,\n",
       " 'snow_precip': 4.02112619e-06,\n",
       " 'wind_direction': 291.185028,\n",
       " 'wind_speed': 1.36716318}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_csv_backup[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd225c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ae7383de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3935"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8e322b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[row['Clip Path'] for row in dataset_csv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4058b400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3811, 3700)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len((a)),len(set(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "95d8f3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_version': 'W2', 'Annotator': 'weather_V2', 'Site ID': '03', 'File Name': '/tank/data/nna/real/dalton/03/2019/S4A10437_20191009_172000.aac', 'Start Time': '17:20:00.000000', 'End Time': '17:20:10.000000', 'Date': '10/09/2019', 'Length': '00:00:10.000000', 'Clip Path': PosixPath('/scratch/enis/data/nna/labeling/samples/weather_V2/audio_weather_V2/dalton/03/S4A10437_20191009_172000_0m_0s__0m_10s.wav'), 'Comments': '', 'weather_timestamp': '2019-10-09_16:30:00', 'region': 'dalton', 'day_length': 9.85344791, 'air_temp': -0.127105713, 'snow_depth': 0.0195905175, 'cloud_fraction': 0.831612885, 'relative_humidity': 95.3483429, 'runoff': 0.0, 'rain_precip': 1.14912109e-05, 'snow_precip': 7.56504087e-05, 'total_precip': 8.71416196e-05, 'wind_direction': 192.373138, 'wind_speed': 1.04121852, 'snow_blowing_ground': 0.0, 'snow_blowing_air': 0.0787686035}\n"
     ]
    }
   ],
   "source": [
    "for row in dataset_csv:\n",
    "    m = row['Clip Path']\n",
    "    if m.exists():\n",
    "        continue\n",
    "    else:\n",
    "        print(row)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cbe570",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soundenv3",
   "language": "python",
   "name": "soundenv3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
