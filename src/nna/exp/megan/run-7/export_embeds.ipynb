{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchaudio\n",
    "torchaudio.set_audio_backend('sox_io')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nna import dataimport\n",
    "from nna import fileUtils\n",
    "from nna.exp import runutils\n",
    "\n",
    "import modelarchs  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def repeat_data(data, expected_len) -> np.ndarray:\n",
    "    '''pad by zeros if it is not divisible expected_len seconds.\n",
    "    '''\n",
    "    sr = 48000\n",
    "    left_over = (data.shape[0]) % (expected_len * sr)\n",
    "\n",
    "    if left_over != 0:\n",
    "        missing_element_count = (expected_len * sr) - left_over\n",
    "        padded_data = np.pad(data[-left_over:], (0, missing_element_count),\n",
    "                             'constant',\n",
    "                             constant_values=(0, 0))\n",
    "        return np.concatenate([data[:-left_over], padded_data])  # type: ignore\n",
    "    else:\n",
    "        return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pathMap():\n",
    "    def __init__(self) -> None:\n",
    "        scratch = '/scratch/enis/data/nna/'\n",
    "        home = '/home/enis/projects/nna/'\n",
    "        self.exp_dir = '/home/enis/projects/nna/src/nna/exp/megan/run-5/'\n",
    "        self.clipping_results_path = (scratch +\n",
    "                                         'clipping_info/all-merged_2021-02-10/')\n",
    "        self.output_dir = scratch + 'real/'\n",
    "        self.file_properties_df_path = scratch + 'database/allFields_dataV4.pkl'\n",
    "        # model_path= ('/home/enis/projects/nna/src/nna/exp/megan/run-3/'+\n",
    "        # 'checkpoints_keep/glorious-sweep-57/best_model_56_ROC_AUC=0.8690.pt')\n",
    "        checkpoints_dir = scratch + 'runs_models/megan/run-5/checkpoints/'\n",
    "        self.model_path = (\n",
    "            checkpoints_dir +\n",
    "            'comic-sweep-29/best_model_20_min_ROC_AUC=0.7202.pt')\n",
    "\n",
    "\n",
    "\n",
    "def setup_inputs(args):\n",
    "    index, count = int(args.index), int(args.count)\n",
    "\n",
    "    region_location = [['dalton', '01'], ['dalton', '02'], ['dalton', '03'],\n",
    "                       ['dalton', '04'], ['dalton', '05'], ['dalton', '06'],\n",
    "                       ['dalton', '07'], ['dalton', '08'], ['dalton', '09'],\n",
    "                       ['dalton', '10'], ['dempster', '11'], ['dempster', '12'],\n",
    "                       ['dempster', '13'], ['dempster',\n",
    "                                            '14'], ['dempster', '16'],\n",
    "                       ['dempster', '17'], ['dempster',\n",
    "                                            '19'], ['dempster', '20'],\n",
    "                       ['dempster', '21'], ['dempster',\n",
    "                                            '22'], ['dempster', '23'],\n",
    "                       ['dempster', '24'], ['dempster', '25'],\n",
    "                       ['ivvavik', 'AR01'], ['ivvavik', 'AR02'],\n",
    "                       ['ivvavik', 'AR03'], ['ivvavik', 'AR04'],\n",
    "                       ['ivvavik', 'AR05'], ['ivvavik', 'AR06'],\n",
    "                       ['ivvavik', 'AR07'], ['ivvavik', 'AR08'],\n",
    "                       ['ivvavik', 'AR09'], ['ivvavik', 'AR10'],\n",
    "                       ['ivvavik', 'SINP01'], ['ivvavik', 'SINP02'],\n",
    "                       ['ivvavik', 'SINP03'], ['ivvavik', 'SINP04'],\n",
    "                       ['ivvavik', 'SINP05'], ['ivvavik', 'SINP06'],\n",
    "                       ['ivvavik', 'SINP07'], ['ivvavik', 'SINP08'],\n",
    "                       ['ivvavik', 'SINP09'], ['ivvavik', 'SINP10']]\n",
    "\n",
    "    return region_location[index:index + count]\n",
    "\n",
    "\n",
    "def setup(args):\n",
    "\n",
    "    pathmap = pathMap()\n",
    "\n",
    "    os.chdir(pathmap.exp_dir)\n",
    "\n",
    "    file_properties_df = pd.read_pickle(pathmap.file_properties_df_path)\n",
    "\n",
    "    device = 'cuda:' + str(args.gpu)\n",
    "    device = torch.device(device)\n",
    "\n",
    "    CATEGORY_COUNT = 9\n",
    "    # '1.1.10','1.1.7'\n",
    "    maxMelLen = 938\n",
    "    ToTensor_ins = modelarchs.ToTensor(maxMelLen, 48000, device)\n",
    "    transformCompose = transforms.Compose([\n",
    "        ToTensor_ins,\n",
    "    ])\n",
    "\n",
    "    h_w = [128, 938]\n",
    "\n",
    "    config = {}\n",
    "    config['label_names'] = ['1-0-0',\n",
    "                            '1-1-0',\n",
    "                            '1-1-10',\n",
    "                            '1-1-7',\n",
    "                            '0-0-0',\n",
    "                            '1-3-0',\n",
    "                            '1-1-8',\n",
    "                            '0-2-0',\n",
    "                            '3-0-0',\n",
    "                            ]\n",
    "    config['v_str'] = 'multi9-V1'\n",
    "    config['fc_1_size'] = 64\n",
    "    config['CNN_kernel_size'] = 12\n",
    "    config['CNN_filters_1'] = 6\n",
    "    config['expected_len'] = 10\n",
    "    config['device'] = device\n",
    "\n",
    "    output_shape = (CATEGORY_COUNT,)\n",
    "\n",
    "    model_saved = modelarchs.singleconv1dModel(\n",
    "        out_channels=config['CNN_filters_1'],\n",
    "        h_w=(1, h_w[0] * h_w[1]),\n",
    "        fc_1_size=config['fc_1_size'],\n",
    "        kernel_size=config['CNN_kernel_size'],\n",
    "        output_shape=output_shape)\n",
    "\n",
    "    model_saved.load_state_dict(\n",
    "        torch.load(pathmap.model_path, map_location=config['device']))\n",
    "    model_saved.eval().to(config['device'])\n",
    "\n",
    "    return model_saved, transformCompose, config, file_properties_df, pathmap\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader_from_audio_ins(audio_ins, config, transformCompose):\n",
    "    audio_ins.load_data()\n",
    "    audio_ins.pick_channel_by_clipping(config['expected_len'])\n",
    "    input_file_data = repeat_data(audio_ins.data, config['expected_len'])\n",
    "\n",
    "    # divide to 10 second excerpts\n",
    "    input_file_data = input_file_data.reshape(-1, 480000)\n",
    "    input_file_data = torch.from_numpy(input_file_data).float()\n",
    "    dataset = {\n",
    "        'predict':\n",
    "            runutils.audioDataset(input_file_data,\n",
    "                                  None,\n",
    "                                  transform=transformCompose)\n",
    "    }\n",
    "    dataloader = {\n",
    "        'predict':\n",
    "            torch.utils.data.DataLoader(dataset['predict'],\n",
    "                                        shuffle=False,\n",
    "                                        batch_size=1)\n",
    "    }\n",
    "    return dataloader\n",
    "\n",
    "def prepare_dataloader_from_array(input_data):\n",
    "\n",
    "    # divide to 10 second excerpts\n",
    "    input_file_data = input_data.reshape(-1, 480000)\n",
    "    input_file_data = torch.from_numpy(input_file_data).float()\n",
    "    dataset = {\n",
    "        'predict':\n",
    "            runutils.audioDataset(input_file_data,\n",
    "                                  None,\n",
    "                                  transform=transformCompose)\n",
    "    }\n",
    "    dataloader = {\n",
    "        'predict':\n",
    "            torch.utils.data.DataLoader(dataset['predict'],\n",
    "                                        shuffle=False,\n",
    "                                        batch_size=1)\n",
    "    }\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "\n",
    "def single_file_inference(dataloader, config, model_saved):\n",
    "    outputs = []\n",
    "    for inputs, labels in dataloader['predict']:\n",
    "        del labels\n",
    "        inputs = inputs.float().to(config['device'])\n",
    "        output = model_saved(inputs)\n",
    "        output = output.to('cpu')\n",
    "        index = output.data.numpy()\n",
    "        outputs.append(index)\n",
    "    outputs = np.concatenate(outputs)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def save_results_disk(outputs, audio_ins, label_names, v_str,\n",
    "                      file_properties_df, pathmap):\n",
    "    # label_names = ['1-1-10', '1-1-7']\n",
    "    # v_str = 'V3'\n",
    "    file_names = output_file_names(audio_ins, label_names, v_str,\n",
    "                                   file_properties_df, pathmap)\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        file_name.parent.mkdir(parents=True, exist_ok=True)\n",
    "        file_name = file_name.with_suffix('.npy')\n",
    "        np.save(str(file_name), outputs[:, i])\n",
    "    audio_ins.data = None\n",
    "\n",
    "\n",
    "def output_file_names(audio_ins, label_names, v_str, file_properties_df,\n",
    "                      pathmap):\n",
    "    row = file_properties_df.loc[audio_ins.path]\n",
    "    file_names = []\n",
    "    for _, label_name in enumerate(label_names):\n",
    "        sub_directory_addon = v_str + '-' + label_name\n",
    "        file_name_addon = sub_directory_addon\n",
    "        file_name = fileUtils.standard_path_style(\n",
    "            pathmap.output_dir,\n",
    "            row,\n",
    "            sub_directory_addon=sub_directory_addon,\n",
    "            file_name_addon=file_name_addon)\n",
    "        #             print(file_name)\n",
    "        file_names.append(file_name)\n",
    "    return file_names\n",
    "\n",
    "\n",
    "def is_result_exist(audio_ins, label_names, v_str, file_properties_df, pathmap):\n",
    "    file_names = output_file_names(audio_ins, label_names, v_str,\n",
    "                                   file_properties_df, pathmap)\n",
    "    for file_name in file_names:\n",
    "        file_name = file_name.with_suffix('.npy')\n",
    "        if not file_name.exists():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# def main(args):\n",
    "#     region_location = setup_inputs(args)\n",
    "#     model_saved, transformCompose, config, file_properties_df, pathmap = setup(\n",
    "#         args)\n",
    "#     region_location_datasets = load_audio_files(region_location,\n",
    "#                                                 file_properties_df)\n",
    "#     label_names = config['label_names']\n",
    "#     v_str = config['v_str']\n",
    "#     for region, location in region_location:\n",
    "#         print(region, location)\n",
    "#         region_location_ins = region_location_datasets[(region, location)]\n",
    "#         region_location_ins.update_samples_w_clipping_info(\n",
    "#             output_folder=pathmap.clipping_results_path)\n",
    "#         # print('inference part')\n",
    "#         for audio_ins in region_location_ins.values():\n",
    "#             if is_result_exist(audio_ins, label_names, v_str,\n",
    "#                                file_properties_df, pathmap):\n",
    "#                 continue\n",
    "#             dataloader = prepare_dataloader_from_audio_ins(\n",
    "#                 audio_ins, config, transformCompose)\n",
    "#             outputs = single_file_inference(dataloader, config, model_saved)\n",
    "#             save_results_disk(outputs, audio_ins, label_names, v_str,\n",
    "#                               file_properties_df, pathmap)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self,index,count,gpu):\n",
    "        self.index=index\n",
    "        self.count=count\n",
    "        self.gpu=gpu\n",
    "\n",
    "args = Arguments(0,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region_location = setup_inputs(args)\n",
    "model_saved, transformCompose, config, file_properties_df, pathmap = setup(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_audio_files(region_location, file_properties_df):\n",
    "#     '''Load audio files from given regions and locations as Audio dataset\n",
    "#     '''\n",
    "#     region_location_datasets = {}\n",
    "#     for region, location in region_location:\n",
    "#         filtered_files = file_properties_df[file_properties_df.region == region]\n",
    "#         filtered_files = filtered_files[filtered_files.locationId == location]\n",
    "#         filtered_files = filtered_files[filtered_files.durationSec > 0]\n",
    "#         dataset_name_v = '-'.join([region, location])\n",
    "#         audio_dataset = dataimport.Dataset(dataset_name_v=dataset_name_v)\n",
    "#         for i in filtered_files.iterrows():\n",
    "#             audio_dataset[i[0]] = dataimport.Audio(i[1].name,\n",
    "#                                                    float(i[1].durationSec))\n",
    "#         region_location_datasets[(region, location)] = audio_dataset\n",
    "#     return region_location_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/enis/data/nna/labeling/megan/AudioSamplesPerSite/meganLabeledFiles_wlenV1.txt\n",
      "/scratch/enis/data/nna/labeling/megan/AudioSamplesPerSite\n",
      "4 files are missing corresponding to excell entries\n",
      "'-> 5 number of samples are DELETED due to ignore_files and missing_audio_files'\n",
      "-> 415 samples DELETED because they are not in the excell\n",
      "\n",
      "-> 0 samples DELETED because they do not have the taxo info coming from excell\n",
      "\n",
      "-> classes that do not have enough data:\n",
      "[REMOVED!]\n",
      "\n",
      "-> classes that have enough data:\n",
      "['other-biophony'] 56.0\n",
      "['other-insect'] 140.0\n",
      "['other-bird'] 661.0\n",
      "['songbirds'] 392.0\n",
      "['duck-goose-swan'] 183.0\n",
      "['grouse-ptarmigan'] 59.0\n",
      "['other-anthrophony'] 66.0\n",
      "['other-mammal'] 0.0\n",
      "['other-silence'] 20.0\n",
      "['unknown-sound'] 2.0\n",
      "['other-aircraft'] 107.0\n",
      "['seabirds'] 1.0\n",
      "['canids'] 1.0\n",
      "['loons'] 29.0\n",
      "['other-car'] 37.0\n",
      "['other-flare'] 11.0\n",
      "['other-rain'] 20.0\n",
      "('-> 0 number of samples are deleted because their taxonomy category does not '\n",
      " 'have enough data.')\n",
      "-> classes that do not have enough data\n",
      "will be REMOVED!\n",
      "-> 97 number of samples are deleted because their length is not long enough.\n",
      "loading from cache at /scratch/enis/data/nna/labeling/megan/AudioSamplesPerSite/files_as_np_filtered_v3_int16.pkl\n"
     ]
    }
   ],
   "source": [
    "audio_dataset, _ = run.prepare_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCERPT_LENGTH=10\n",
    "audio_dataset = run.dataset_generate_samples(audio_dataset,EXCERPT_LENGTH)\n",
    "all_taxo= list({sound_ins.taxo_code for sound_ins in audio_dataset.values()})\n",
    "\n",
    "x_data, y_data, location_id_info = run.put_samples_into_array(\n",
    "    all_taxo, [], audio_dataset)\n",
    "x_data=np.array(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=list(iter(audio_dataset.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "timestamps=[]\n",
    "file_paths = []\n",
    "audio_index=[]\n",
    "for sound_ins in audio_dataset.values():\n",
    "    i=0\n",
    "    for sample in sound_ins.samples:\n",
    "        \n",
    "        day,clock = (sound_ins.name.split('_')[1:3])\n",
    "        year,month,day = day[:4],day[4:6],day[6:8]\n",
    "        hour,minute,second = clock[:2],clock[2:4],clock[4:6]\n",
    "        year,month,day=int(year),int(month),int(day)\n",
    "        hour,minute,second=int(hour),int(minute),int(second)\n",
    "        timestamp = datetime.datetime(year,\n",
    "                                  month,\n",
    "                                  day,\n",
    "                                  hour=hour,\n",
    "                                  minute=minute,\n",
    "                                  second=second,\n",
    "                                  microsecond=0)\n",
    "        timestamp=timestamp+datetime.timedelta(seconds=10*i)\n",
    "        timestamps.append(timestamp)\n",
    "        audio_index.append(i)\n",
    "        file_paths.append(str(sound_ins.path))\n",
    "        i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S4A10226_20190709_131602_Bio_Bug18_part-3082.wav'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     dataloader = prepare_dataloader_from_audio_ins(\n",
    "#         audio_ins, config, transformCompose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     outputs = single_file_inference(dataloader, config, model_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data=np.array(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader=prepare_dataloader_from_array(x_data[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = single_file_inference(dataloader, config, model_saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(data):\n",
    "    return 1 / (1 + np.exp(-data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3083, 480000)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def single_file_inference(dataloader, config, model_saved):\n",
    "outputs = []\n",
    "activations=[]\n",
    "for inputs, labels in dataloader['predict']:\n",
    "#     print(labels)\n",
    "    del labels\n",
    "    inputs = inputs.float().to(config['device'])\n",
    "\n",
    "    output = model_saved(inputs)\n",
    "    output = output.to('cpu')\n",
    "    index = output.data.numpy()\n",
    "    outputs.append(index)\n",
    "    activations.append(activation['fc1'])\n",
    "# outputs.append(index)\n",
    "#     outputs = np.concatenate(outputs)\n",
    "#     return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=sigmoid(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.7977953,  -1.8959762,  -0.9214242,  -7.415851 ,  -4.75553  ,\n",
       "        -1.4654305,  -8.198331 , -12.866567 ,  -7.023336 ], dtype=float32)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/enis/conda/envs/soundenv3/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "outputs_list=[]\n",
    "for out in outputs:\n",
    "    out_str=[ f\"{x:.2f}\" for x in list(sigmoid(out[0]))]\n",
    "    outputs_list.append(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3083"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = MyModel()\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model_saved.fc1.register_forward_hook(get_activation('fc1'))\n",
    "# x = torch.randn(1, 25)\n",
    "# output = model(x)\n",
    "# print(activation['fc2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "activationsnp=[]\n",
    "for act in activations:\n",
    "    act = act.to('cpu')\n",
    "    act = act.data.numpy()\n",
    "    activationsnp.append(act)\n",
    "\n",
    "activationsnp=np.array(activationsnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/scratch/enis/data/nna/labeling/megan/AudioSamplesPerSite_post/run-5_comic-sweep-29_activations-fc1.npy', 'wb') as f:\n",
    "    np.save(f,activationsnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(activationsnp),y_data, location_id_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_per_set = [[\n",
    "    '12', '14', '27', '49', '31', '39', '44', '45', '48', '19', '16', '21',\n",
    "    '38', '41', '20', '29', '37', '15'\n",
    "], ['17', '46', '50', '32', '33', '25', '40'],\n",
    "               ['11', '18', '34', '24', '13', '22', '36', '47', '30']]\n",
    "\n",
    "target_taxo = [\n",
    "    '1.0.0', '1.1.0', '1.1.10', '1.1.7', '0.0.0', '1.3.0', '1.1.8', '0.2.0',\n",
    "    '3.0.0'\n",
    "]\n",
    "multi_label_vector = run.create_multi_label_vector(target_taxo, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc_per_set_find(loc,loc_per_set):\n",
    "    i2name={0:'train',1:'test',2:'valid'}\n",
    "    for i,m in enumerate(loc_per_set):\n",
    "        if loc in m:\n",
    "            return i2name[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_names=[loc_per_set_find(loc,loc_per_set) for loc in location_id_info]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 0, 0, 0, 0, 0, 0, 0, 0], 'valid')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_label_vector[0],set_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_taxo_names=[audio_dataset.taxonomy[taxo][0].replace('other-','') for taxo in target_taxo]\n",
    "target_taxo_names_header=('_pred,'.join(target_taxo_names))+'_pred'\n",
    "target_taxo_header = (','.join(target_taxo_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('biophony_pred,bird_pred,songbirds_pred,duck-goose-swan_pred,anthrophony_pred,insect_pred,grouse-ptarmigan_pred,aircraft_pred,silence_pred',\n",
       " 'biophony,bird,songbirds,duck-goose-swan,anthrophony,insect,grouse-ptarmigan,aircraft,silence')"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_taxo_names_header,target_taxo_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_index,file_paths\n",
    "# with open('/scratch/enis/data/nna/labeling/megan/AudioSamplesPerSite_post/run-5_comic-sweep-29_activations-fc1_labels_v2.csv','w') as f:\n",
    "#     f.writelines('sample_id,file_path,part_index,set_name,location_id,timestamp,'+(','.join(target_taxo))+'\\n')\n",
    "#     for i,label_vector in enumerate(multi_label_vector):\n",
    "#         label_vector = [str(i) for i in label_vector]\n",
    "#         t = timestamps[i].strftime('%Y-%m-%d_%H:%M:%S')\n",
    "#         f.writelines(f'{i},{file_paths[i]},{audio_index[i]},{t},{location_id_info[i]},{set_names[i]},'+(','.join(label_vector))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '0', '0', '0', '0', '1', '0', '0', '0']"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_index,file_paths\n",
    "with open('/scratch/enis/data/nna/labeling/megan/AudioSamplesPerSite_post/run-5_comic-sweep-29_activations-fc1_labels_v3.csv','w') as f:\n",
    "    f.writelines('sample_id,file_path,part_index,timestamp,location_id,set_name,'+target_taxo_header+','+target_taxo_names_header+'\\n')\n",
    "    for i,label_vector in enumerate(multi_label_vector):\n",
    "        label_vector = [str(i) for i in label_vector]\n",
    "        label_vector=','.join(label_vector)\n",
    "        pred_vector = outputs_list[i]\n",
    "        \n",
    "        t = timestamps[i].strftime('%Y-%m-%d_%H:%M:%S')\n",
    "        f.writelines(f'{i},{file_paths[i]},{audio_index[i]},{t},{location_id_info[i]},{set_names[i]},{(label_vector)},'+(','.join(pred_vector))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nna.exp.megan import preparedataset\n",
    "preparedataset.load_taxonomy2dataset(('/home/enis/projects/nna/src/nna/assets/taxonomy/taxonomy.yaml'), audio_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'biophony'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_dataset.update_samples_w_clipping_info(\n",
    "#     output_folder=pathmap.clipping_results_path)\n",
    "# # print('inference part')\n",
    "# for audio_ins in audio_dataset.values():\n",
    "# #     if is_result_exist(audio_ins, label_names, v_str,\n",
    "# #                        file_properties_df, pathmap):\n",
    "# #         continue\n",
    "#     dataloader = prepare_dataloader_from_audio_ins(\n",
    "#         audio_ins, config, transformCompose)\n",
    "#     outputs = single_file_inference(dataloader, config, model_saved)\n",
    "#     break\n",
    "    \n",
    "# #     save_results_disk(outputs, audio_ins, label_names, v_str,\n",
    "# #                       file_properties_df, pathmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_location_datasets = load_audio_files(region_location,\n",
    "                                            file_properties_df)\n",
    "label_names = config['label_names']\n",
    "v_str = config['v_str']\n",
    "# for region, location in region_location:\n",
    "#     print(region, location)\n",
    "#     region_location_ins = region_location_datasets[(region, location)]\n",
    "    region_location_ins.update_samples_w_clipping_info(\n",
    "        output_folder=pathmap.clipping_results_path)\n",
    "    # print('inference part')\n",
    "    for audio_ins in region_location_ins.values():\n",
    "        if is_result_exist(audio_ins, label_names, v_str,\n",
    "                           file_properties_df, pathmap):\n",
    "            continue\n",
    "        dataloader = prepare_dataloader_from_audio_ins(\n",
    "            audio_ins, config, transformCompose)\n",
    "        outputs = single_file_inference(dataloader, config, model_saved)\n",
    "        save_results_disk(outputs, audio_ins, label_names, v_str,\n",
    "                          file_properties_df, pathmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another thing\n",
    "# with open('/scratch/enis/data/nna/labeling/megan/AudioSamplesPerSite/files_filtered_v3_info.csv','w') as f:\n",
    "#     f.writelines(f'file_name,taxo_code,location_id\\n')\n",
    "#     for file_name in cached_dict.keys():\n",
    "# #         print(file_name)\n",
    "#         audio=audio_dataset[file_name.split('/')[-1]]\n",
    "#         taxo_code=audio.taxo_code\n",
    "#         location_id=audio.location_id\n",
    "        \n",
    "#         f.writelines(f'{file_name},{taxo_code},{location_id}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soundenv3",
   "language": "python",
   "name": "soundenv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
