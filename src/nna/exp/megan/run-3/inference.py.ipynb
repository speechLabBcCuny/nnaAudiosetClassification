{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython import get_ipython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchaudio\n",
    "torchaudio.set_audio_backend('sox_io')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nna.exp import runutils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nna import dataimport\n",
    "from nna import fileUtils\n",
    "from nna.exp import runutils\n",
    "\n",
    "import modelarchs  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def repeat_data(data, expected_len) -> np.ndarray:\n",
    "    '''pad by zeros if it is not divisible expected_len seconds.\n",
    "    '''\n",
    "    sr = 48000\n",
    "    left_over = (data.shape[0]) % (expected_len * sr)\n",
    "\n",
    "    if left_over != 0:\n",
    "        missing_element_count = (expected_len * sr) - left_over\n",
    "        padded_data = np.pad(data[-left_over:], (0, missing_element_count),\n",
    "                             'constant',\n",
    "                             constant_values=(0, 0))\n",
    "        return np.concatenate([data[:-left_over], padded_data])  # type: ignore\n",
    "    else:\n",
    "        return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pathMap():\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pathMap.exp_dir = '/home/enis/projects/nna/src/nna/exp/megan/run-3/'\n",
    "        pathMap.clipping_results_path = '/scratch/enis/data/nna/clipping_info/all_data_2021-02-08/'\n",
    "        pathMap.output_dir = '/scratch/enis/data/nna/real/'\n",
    "        pathMap.file_properties_df_path = '/scratch/enis/data/nna/database/allFields_dataV4.pkl'\n",
    "        # model_path='/home/enis/projects/nna/src/nna/exp/megan/run-3/checkpoints_keep/glorious-sweep-57/best_model_56_ROC_AUC=0.8690.pt'\n",
    "        checkpoints_dir = '/scratch/enis/data/nna/runs_models/megan/run-3/checkpoints/'\n",
    "        pathMap.model_path = checkpoints_dir + 'iconic-salad-1010/best_model_79_ROC_AUC=0.8564.pt'\n",
    "\n",
    "\n",
    "def setup_inputs(args):\n",
    "    index, count = int(args.index), int(args.count)\n",
    "\n",
    "    region_location = [['dalton', '01'], ['dalton', '02'], ['dalton', '03'],\n",
    "                       ['dalton', '04'], ['dalton', '05'], ['dalton', '06'],\n",
    "                       ['dalton', '07'], ['dalton', '08'], ['dalton', '09'],\n",
    "                       ['dalton', '10'], ['dempster', '11'], ['dempster', '12'],\n",
    "                       ['dempster', '13'], ['dempster',\n",
    "                                            '14'], ['dempster', '16'],\n",
    "                       ['dempster', '17'], ['dempster',\n",
    "                                            '19'], ['dempster', '20'],\n",
    "                       ['dempster', '21'], ['dempster',\n",
    "                                            '22'], ['dempster', '23'],\n",
    "                       ['dempster', '24'], ['dempster', '25'],\n",
    "                       ['ivvavik', 'AR01'], ['ivvavik', 'AR02'],\n",
    "                       ['ivvavik', 'AR03'], ['ivvavik', 'AR04'],\n",
    "                       ['ivvavik', 'AR05'], ['ivvavik', 'AR06'],\n",
    "                       ['ivvavik', 'AR07'], ['ivvavik', 'AR08'],\n",
    "                       ['ivvavik', 'AR09'], ['ivvavik', 'AR10'],\n",
    "                       ['ivvavik', 'SINP01'], ['ivvavik', 'SINP02'],\n",
    "                       ['ivvavik', 'SINP03'], ['ivvavik', 'SINP04'],\n",
    "                       ['ivvavik', 'SINP05'], ['ivvavik', 'SINP06'],\n",
    "                       ['ivvavik', 'SINP07'], ['ivvavik', 'SINP08'],\n",
    "                       ['ivvavik', 'SINP09'], ['ivvavik', 'SINP10']]\n",
    "\n",
    "    return region_location[index:index + count]\n",
    "\n",
    "\n",
    "def setup(args):\n",
    "\n",
    "    pathmap = pathMap()\n",
    "\n",
    "    os.chdir(pathmap.exp_dir)\n",
    "\n",
    "    file_properties_df = pd.read_pickle(pathmap.file_properties_df_path)\n",
    "\n",
    "    device = 'cuda:' + str(args.gpu)\n",
    "    device = torch.device(device)\n",
    "\n",
    "    CATEGORY_COUNT = 2\n",
    "    # '1.1.10','1.1.7'\n",
    "    maxMelLen = 938\n",
    "    ToTensor_ins = modelarchs.ToTensor(maxMelLen, 48000, device)\n",
    "    transformCompose = transforms.Compose([\n",
    "        ToTensor_ins,\n",
    "    ])\n",
    "\n",
    "    h_w = [128, 938]\n",
    "\n",
    "    config = {}\n",
    "    config['label_names'] = ['1-1-10', '1-1-7']\n",
    "    config['v_str'] = 'V3'\n",
    "    config['fc_1_size'] = 120\n",
    "    config['CNN_kernel_size'] = 8\n",
    "    config['CNN_filters_1'] = 2\n",
    "    config['expected_len'] = 10\n",
    "    config['device'] = device\n",
    "\n",
    "    output_shape = (CATEGORY_COUNT,)\n",
    "\n",
    "    model_saved = modelarchs.singleconv1dModel(\n",
    "        out_channels=config['CNN_filters_1'],\n",
    "        h_w=(1, h_w[0] * h_w[1]),\n",
    "        fc_1_size=config['fc_1_size'],\n",
    "        kernel_size=config['CNN_kernel_size'],\n",
    "        output_shape=output_shape)\n",
    "\n",
    "    model_saved.load_state_dict(\n",
    "        torch.load(pathmap.model_path, map_location=config['device']))\n",
    "    model_saved.eval().to(config['device'])\n",
    "\n",
    "    return model_saved, transformCompose, config, file_properties_df, pathmap\n",
    "\n",
    "\n",
    "def load_audio_files(region_location, file_properties_df):\n",
    "    '''Load audio files from given regions and locations as Audio dataset\n",
    "    '''\n",
    "    region_location_datasets = {}\n",
    "    for region, location in region_location:\n",
    "        filtered_files = file_properties_df[file_properties_df.region == region]\n",
    "        filtered_files = filtered_files[filtered_files.locationId == location]\n",
    "        filtered_files = filtered_files[filtered_files.durationSec > 0]\n",
    "        dataset_name_v = '-'.join([region, location])\n",
    "        audio_dataset = dataimport.Dataset(dataset_name_v=dataset_name_v)\n",
    "        for i in filtered_files.iterrows():\n",
    "            audio_dataset[i[0]] = dataimport.Audio(i[1].name,\n",
    "                                                   float(i[1].durationSec))\n",
    "        region_location_datasets[(region, location)] = audio_dataset\n",
    "    return region_location_datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader_from_audio_ins(audio_ins, config, transformCompose):\n",
    "    audio_ins.load_data()\n",
    "    audio_ins.pick_channel_by_clipping(config['expected_len'])\n",
    "    input_file_data = repeat_data(audio_ins.data, config['expected_len'])\n",
    "\n",
    "    # divide to 10 second excerpts\n",
    "    input_file_data = input_file_data.reshape(-1, 480000)\n",
    "    input_file_data = torch.from_numpy(input_file_data).float()\n",
    "    dataset = {\n",
    "        'predict':\n",
    "            runutils.audioDataset(input_file_data,\n",
    "                                  None,\n",
    "                                  transform=transformCompose)\n",
    "    }\n",
    "    dataloader = {\n",
    "        'predict':\n",
    "            torch.utils.data.DataLoader(dataset['predict'],\n",
    "                                        shuffle=False,\n",
    "                                        batch_size=128)\n",
    "    }\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def single_file_inference(dataloader, config, model_saved):\n",
    "    outputs = []\n",
    "    for inputs, labels in dataloader['predict']:\n",
    "        del labels\n",
    "        inputs = inputs.float().to(config['device'])\n",
    "        output = model_saved(inputs)\n",
    "        output = output.to('cpu')\n",
    "        index = output.data.numpy()\n",
    "        outputs.append(index)\n",
    "    outputs = np.concatenate(outputs)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def save_results_disk(outputs, audio_ins, label_names, v_str,\n",
    "                      file_properties_df, pathmap):\n",
    "    # label_names = ['1-1-10', '1-1-7']\n",
    "    # v_str = 'V3'\n",
    "    file_names = output_file_names(audio_ins, label_names, v_str,\n",
    "                                   file_properties_df, pathmap)\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        file_name.parent.mkdir(parents=True, exist_ok=True)\n",
    "        file_name = file_name.with_suffix('.npy')\n",
    "        np.save(str(file_name), outputs[:, i])\n",
    "    audio_ins.data = None\n",
    "\n",
    "\n",
    "def output_file_names(audio_ins, label_names, v_str, file_properties_df,\n",
    "                      pathmap):\n",
    "    row = file_properties_df.loc[audio_ins.path]\n",
    "    file_names = []\n",
    "    for _, label_name in enumerate(label_names):\n",
    "        sub_directory_addon = v_str + '-' + label_name\n",
    "        file_name_addon = sub_directory_addon\n",
    "        file_name = fileUtils.standard_path_style(\n",
    "            pathmap.output_dir,\n",
    "            row,\n",
    "            sub_directory_addon=sub_directory_addon,\n",
    "            file_name_addon=file_name_addon)\n",
    "        #             print(file_name)\n",
    "        file_names.append(file_name)\n",
    "    return file_names\n",
    "\n",
    "\n",
    "def is_result_exist(audio_ins, label_names, v_str, file_properties_df, pathmap):\n",
    "    file_names = output_file_names(audio_ins, label_names, v_str,\n",
    "                                   file_properties_df, pathmap)\n",
    "    for file_name in file_names:\n",
    "        file_name = file_name.with_suffix('.npy')\n",
    "        if not file_name.exists():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    region_location = setup_inputs(args)\n",
    "    model_saved, transformCompose, config, file_properties_df, pathmap = setup(\n",
    "        args)\n",
    "    region_location_datasets = load_audio_files(region_location,\n",
    "                                                file_properties_df)\n",
    "    label_names = config['label_names']\n",
    "    v_str = config['v_str']\n",
    "    for region, location in region_location:\n",
    "        print(region, location)\n",
    "        region_location_ins = region_location_datasets[(region, location)]\n",
    "        region_location_ins.update_samples_w_clipping_info(\n",
    "            output_folder=pathmap.clipping_results_path)\n",
    "        # print('inference part')\n",
    "        for audio_ins in region_location_ins.values():\n",
    "            if is_result_exist(audio_ins, label_names, v_str,\n",
    "                               file_properties_df, pathmap):\n",
    "                continue\n",
    "            dataloader = prepare_dataloader_from_audio_ins(\n",
    "                audio_ins, config, transformCompose)\n",
    "            outputs = single_file_inference(dataloader, config, model_saved)\n",
    "            save_results_disk(outputs, audio_ins, label_names, v_str,\n",
    "                              file_properties_df, pathmap)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--index', help='index of array', required=True)\n",
    "    parser.add_argument('--count', help='count of items', required=True)\n",
    "    parser.add_argument('-g', '--gpu', help='gpu index', type=int, default=1)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soundenv3",
   "language": "python",
   "name": "soundenv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}